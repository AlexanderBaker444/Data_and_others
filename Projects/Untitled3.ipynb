{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'abstract_tokens.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-02561aac36b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m#     p.dump(text_tokens,handle)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"abstract_tokens.p\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mtext_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'abstract_tokens.p'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "import heapq\n",
    "import tensorflow as tf\n",
    "import pickle as p\n",
    "\n",
    "df=pd.read_csv('../data/metadata.csv')\n",
    "df[\"publish_year\"] = list(map(lambda date: int(date[:4]) if type(date)==str else 0,df['publish_time']))\n",
    "df = df[df[\"publish_year\"]>=2000].reset_index(drop=True)\n",
    "abstracts=df['abstract']\n",
    "\n",
    "ngram_count = 1\n",
    "num_words_to_keep = 20000\n",
    "num_ngrams_to_keep = 15000\n",
    "\n",
    "def string_cleaner(text):\n",
    "    # Clean the documents\n",
    "    stop = set(stopwords.words('english') + stopwords.words('spanish') + stopwords.words('french'))\n",
    "    exclude = string.punctuation\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    start_strip_word = ['abstract', 'background', 'summary', 'objective']\n",
    "    text = str(text).lower() # downcase\n",
    "    for word in start_strip_word:\n",
    "        if text.startswith(word):\n",
    "            text = text[len(word):]\n",
    "    tokens = nltk.tokenize.word_tokenize(text) # split string into words (tokens)\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    tokens = [t for t in tokens if t not in stop] # remove stopwords\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    return tokens\n",
    "\n",
    "# text_tokens = [string_cleaner(abstract) for abstract in abstracts]\n",
    "# with open(\"abstract_tokens.p\",\"wb\") as handle:\n",
    "#     p.dump(text_tokens,handle)\n",
    "\n",
    "with open(\"abstract_tokens.p\",\"rb\") as handle:\n",
    "    text_tokens = p.load(handle)\n",
    "\n",
    "\n",
    "#\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_df = scaler.fit_transform(ngram_count_matrix)\n",
    "# pca = PCA(n_components=100)\n",
    "# pca_df = pca.fit_transform(scaled_df)\n",
    "# print(sum(pca.explained_variance_ratio_))\n",
    "# kmeans = KMeans(n_clusters=10, random_state=0).fit(pca_df)\n",
    "# clusters = kmeans.predict(pca_df)\n",
    "# df[\"pca_clusters\"] = clusters\n",
    "# df['pca_clusters'].value_counts()\n",
    "\n",
    "# #always more types of topic modeling Latent Discriminate Analysis\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "# nmf = NMF(n_components=100)\n",
    "# nnmf_df = nmf.fit_transform(ngram_count_matrix)\n",
    "# from sklearn.cluster import KMeans\n",
    "# kmeans = KMeans(n_clusters=10, random_state=0).fit(nnmf_df)\n",
    "# clusters = kmeans.predict(nnmf_df)\n",
    "#\n",
    "# df[\"nnmf_clusters\"] = clusters\n",
    "# df['nnmf_clusters'].value_counts()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english',max_features=num_words_to_keep,ngram_range=(2,2))\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform([\" \".join(tokens) for tokens in text_tokens])\n",
    "\n",
    "# Helper function (pulled from https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)\n",
    "import seaborn as sns\n",
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts += t.toarray()[0]\n",
    "\n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x: x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words))\n",
    "\n",
    "    plt.figure(2, figsize=(15, 15 / 1.6180))\n",
    "    plt.subplot(title='10 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90)\n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "# Visualise the 10 most common words\n",
    "plot_10_most_common_words(count_data, count_vectorizer)\n",
    "\n",
    "\n",
    "number_topics = 10\n",
    "number_words = 10\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=number_topics, max_iter=100,random_state=0,n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)\n",
    "# lda_df = lda.transform(scaled_df)\n",
    "\n",
    "#\n",
    "# # hierarchal clustering\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# agg=AgglomerativeClustering(n_clusters=10).fit(nnmf_df)\n",
    "# agg_df = agg.transform(nnmf_df)\n",
    "\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import os\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_' + str(number_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath) as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_' + str(number_topics) + '.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
