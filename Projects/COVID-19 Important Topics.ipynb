{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\603766\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "from pytorch_transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
    "from pytorch_transformers import AdamW\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from bert_serving.client import BertClient\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "from tqdm import tqdm, trange\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_full_text</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vho70jcx</td>\n",
       "      <td>f056da9c64fbf00a4645ae326e8a4339d015d155</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>SIANN: Strain Identification by Alignment to N...</td>\n",
       "      <td>10.1101/001727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Next-generation sequencing is increasingly bei...</td>\n",
       "      <td>2014-01-10</td>\n",
       "      <td>Samuel Minot; Stephen D Turner; Krista L Ternu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>biorxiv_medrxiv</td>\n",
       "      <td>https://doi.org/10.1101/001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i9tbix2v</td>\n",
       "      <td>daf32e013d325a6feb80e83d15aabc64a48fae33</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Spatial epidemiology of networked metapopulati...</td>\n",
       "      <td>10.1101/003889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>An emerging disease is one infectious epidemic...</td>\n",
       "      <td>2014-06-04</td>\n",
       "      <td>Lin WANG; Xiang Li</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>biorxiv_medrxiv</td>\n",
       "      <td>https://doi.org/10.1101/003889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62gfisc6</td>\n",
       "      <td>f33c6d94b0efaa198f8f3f20e644625fa3fe10d2</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Sequencing of the human IG light chain loci fr...</td>\n",
       "      <td>10.1101/006866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Germline variation at immunoglobulin gene (IG)...</td>\n",
       "      <td>2014-07-03</td>\n",
       "      <td>Corey T Watson; Karyn Meltz Steinberg; Tina A ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>biorxiv_medrxiv</td>\n",
       "      <td>https://doi.org/10.1101/006866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>058r9486</td>\n",
       "      <td>4da8a87e614373d56070ed272487451266dce919</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Bayesian mixture analysis for metagenomic comm...</td>\n",
       "      <td>10.1101/007476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Deep sequencing of clinical samples is now an ...</td>\n",
       "      <td>2014-07-25</td>\n",
       "      <td>Sofia Morfopoulou; Vincent Plagnol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>biorxiv_medrxiv</td>\n",
       "      <td>https://doi.org/10.1101/007476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wich35l7</td>\n",
       "      <td>eccef80cfbe078235df22398f195d5db462d8000</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Mapping a viral phylogeny onto outbreak trees ...</td>\n",
       "      <td>10.1101/010389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Developing methods to reconstruct transmission...</td>\n",
       "      <td>2014-11-11</td>\n",
       "      <td>Stephen P Velsko; Jonathan E Allen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>biorxiv_medrxiv</td>\n",
       "      <td>https://doi.org/10.1101/010389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha source_x  \\\n",
       "0  vho70jcx  f056da9c64fbf00a4645ae326e8a4339d015d155  biorxiv   \n",
       "1  i9tbix2v  daf32e013d325a6feb80e83d15aabc64a48fae33  biorxiv   \n",
       "2  62gfisc6  f33c6d94b0efaa198f8f3f20e644625fa3fe10d2  biorxiv   \n",
       "3  058r9486  4da8a87e614373d56070ed272487451266dce919  biorxiv   \n",
       "4  wich35l7  eccef80cfbe078235df22398f195d5db462d8000  biorxiv   \n",
       "\n",
       "                                               title             doi pmcid  \\\n",
       "0  SIANN: Strain Identification by Alignment to N...  10.1101/001727   NaN   \n",
       "1  Spatial epidemiology of networked metapopulati...  10.1101/003889   NaN   \n",
       "2  Sequencing of the human IG light chain loci fr...  10.1101/006866   NaN   \n",
       "3  Bayesian mixture analysis for metagenomic comm...  10.1101/007476   NaN   \n",
       "4  Mapping a viral phylogeny onto outbreak trees ...  10.1101/010389   NaN   \n",
       "\n",
       "   pubmed_id  license                                           abstract  \\\n",
       "0        NaN  biorxiv  Next-generation sequencing is increasingly bei...   \n",
       "1        NaN  biorxiv  An emerging disease is one infectious epidemic...   \n",
       "2        NaN  biorxiv  Germline variation at immunoglobulin gene (IG)...   \n",
       "3        NaN  biorxiv  Deep sequencing of clinical samples is now an ...   \n",
       "4        NaN  biorxiv  Developing methods to reconstruct transmission...   \n",
       "\n",
       "  publish_time                                            authors journal  \\\n",
       "0   2014-01-10  Samuel Minot; Stephen D Turner; Krista L Ternu...     NaN   \n",
       "1   2014-06-04                                 Lin WANG; Xiang Li     NaN   \n",
       "2   2014-07-03  Corey T Watson; Karyn Meltz Steinberg; Tina A ...     NaN   \n",
       "3   2014-07-25                 Sofia Morfopoulou; Vincent Plagnol     NaN   \n",
       "4   2014-11-11                 Stephen P Velsko; Jonathan E Allen     NaN   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_full_text   full_text_file  \\\n",
       "0                          NaN            NaN           True  biorxiv_medrxiv   \n",
       "1                          NaN            NaN           True  biorxiv_medrxiv   \n",
       "2                          NaN            NaN           True  biorxiv_medrxiv   \n",
       "3                          NaN            NaN           True  biorxiv_medrxiv   \n",
       "4                          NaN            NaN           True  biorxiv_medrxiv   \n",
       "\n",
       "                              url  \n",
       "0  https://doi.org/10.1101/001727  \n",
       "1  https://doi.org/10.1101/003889  \n",
       "2  https://doi.org/10.1101/006866  \n",
       "3  https://doi.org/10.1101/007476  \n",
       "4  https://doi.org/10.1101/010389  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts=df['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "str_abstract=[str(abstract) for abstract in df['title']]\n",
    "abstract_array = [clean(abstract).split() for abstract in str_abstract] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = abstract_array\n",
    "new_sent=[]\n",
    "for i in sentences:\n",
    "    new_sent.append(str(i))\n",
    "sentences = [sentence + \" [SEP] [CLS]\" for sentence in new_sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', \"'\", 'si', '##ann', \"'\", ',', \"'\", 'strain', \"'\", ',', \"'\", 'identification', \"'\", ',', \"'\", 'alignment', \"'\", ',', \"'\", 'near', \"'\", ',', \"'\", 'neighbor', \"'\", ']', '[SEP]', '[CLS]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer_1 = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts_1 = [tokenizer_1.tokenize(sent) for sent in sentences]\n",
    "print(tokenized_texts_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use kmeans clustering, create and unsupervised model\n",
    "#then input documents into it\n",
    "#sort them into different data frames based on their clutser\n",
    "# make a dictionary and use topic modeling to identify a pattern for each cluster \n",
    "#Do this by identifying the frequency for tokens and comparing it back to \n",
    "#the goal is to idenfiy where our research efforts have been primarily focused \n",
    "# and where most topics remain underresearched "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_1 = [tokenizer_1.convert_tokens_to_ids(x) for x in tokenized_texts_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dict between ids and tokenized texts\n",
    "#see if you can use this in topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_1 = pad_sequences(input_ids_1, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1031,  1005,  9033, ...,     0,     0,     0],\n",
       "       [ 1031,  1005, 13589, ...,     0,     0,     0],\n",
       "       [ 1031,  1005, 24558, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 1031,  1005,  2224, ...,     0,     0,     0],\n",
       "       [ 1031,  1005,  4807, ...,     0,     0,     0],\n",
       "       [ 1031,  1005,  6575, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(input_ids_1)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(input_ids_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.96145525e-02, 2.39774996e-01, 1.73616539e-01, ...,\n",
       "        1.17796257e-06, 1.17794131e-06, 2.10164570e-01],\n",
       "       [1.10961756e-01, 1.22384118e-01, 3.98740022e-01, ...,\n",
       "        8.33179070e-07, 8.33162759e-07, 1.11008163e-01],\n",
       "       [6.08252602e-04, 9.40531867e-03, 2.51375597e-01, ...,\n",
       "        4.22145422e-01, 9.95358886e-02, 1.62639079e-02],\n",
       "       ...,\n",
       "       [3.45735494e-06, 8.65961352e-01, 3.45736616e-06, ...,\n",
       "        3.45726447e-06, 3.45723884e-06, 1.29110989e-01],\n",
       "       [4.21683745e-02, 5.07765620e-02, 7.05272269e-02, ...,\n",
       "        9.28895734e-07, 9.28872530e-07, 3.26024845e-02],\n",
       "       [8.25596292e-03, 1.47579976e-02, 3.74256349e-02, ...,\n",
       "        2.08625725e-01, 3.26193571e-01, 1.62289743e-02]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(input_ids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.82865236e+00, 1.96608269e+00, 0.00000000e+00, ...,\n",
       "        1.44153215e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.54482913e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.33686438e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.01352992e+01, 8.36611611e-01, 1.70106077e+01, ...,\n",
       "        1.25529743e+00, 4.84132567e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [9.31022787e-01, 1.08964680e-02, 0.00000000e+00, ...,\n",
       "        1.94618908e-01, 1.04497700e-03, 2.13075369e-03],\n",
       "       [1.43291463e+00, 2.30576839e+00, 0.00000000e+00, ...,\n",
       "        8.43804519e+00, 0.00000000e+00, 1.89652714e+01],\n",
       "       [2.56263129e+00, 3.50121499e+00, 1.62564640e+01, ...,\n",
       "        2.02994151e+00, 5.47762619e-01, 4.46667307e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.transform(input_ids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=20, random_state=0).fit(input_ids_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 12,  2, ..., 18,  7, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.03100000e+03, 1.00500000e+03, 9.50274466e+03, ...,\n",
       "        1.57113080e+00, 1.26258468e+01, 7.49661282e+00],\n",
       "       [1.03100000e+03, 1.00500000e+03, 8.19159623e+03, ...,\n",
       "        1.90325761e+01, 8.61551650e-01, 8.75696528e-01],\n",
       "       [1.03100000e+03, 1.00500000e+03, 8.93482792e+03, ...,\n",
       "        6.04693141e-01, 4.14380265e+00, 7.15403129e+00],\n",
       "       ...,\n",
       "       [1.03100000e+03, 1.00500000e+03, 9.34227731e+03, ...,\n",
       "        5.01246883e-01, 5.07231920e-01, 5.01246883e-01],\n",
       "       [1.03100000e+03, 1.00500487e+03, 4.88401756e+03, ...,\n",
       "        1.74691465e-01, 1.75560577e-01, 1.74691465e-01],\n",
       "       [1.03100000e+03, 1.00500000e+03, 8.81545695e+03, ...,\n",
       "        1.76814856e+00, 1.91896455e+00, 1.13393360e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = [i for i in doc.lower().split() if i not in stop]\n",
    "    punc_free = [ch for ch in stop_free if ch not in exclude]\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free)\n",
    "    return normalized\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "#dct=Dictionary()\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "dictionary = corpora.Dictionary(abstract_array)\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(abstract) for abstract in abstract_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-ea60d5a5a217>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Running and Trainign LDA model on the document term matrix.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mldamodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    978\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m                     )\n\u001b[1;32m--> 980\u001b[1;33m                     \u001b[0mgammat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[1;34m(self, chunk, state)\u001b[0m\n\u001b[0;32m    740\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m         \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    743\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    694\u001b[0m                 \u001b[0mElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m                 \u001b[0mexpElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mElogthetad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m                 \u001b[0mphinorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpElogthetad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m                 \u001b[1;31m# If gamma hasn't changed much, we're done.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[0mmeanchange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlastgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=10, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "model = Word2Vec(doc_clean, min_count=1)\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
